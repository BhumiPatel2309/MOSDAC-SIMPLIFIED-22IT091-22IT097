{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTeSzGkUFxkn",
        "outputId": "488b4aec-94a9-44b9-e29b-2e4b56dbfc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 selenium lxml pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# --------------------------------------\n",
        "# Clean and extract meaningful text\n",
        "# --------------------------------------\n",
        "def clean_text(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    for script in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        script.extract()\n",
        "    text = soup.get_text(separator=' ', strip=True)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# --------------------------------------\n",
        "# Save content to .txt file\n",
        "# --------------------------------------\n",
        "def save_text(url, text, folder=\"extracted_texts\"):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    filename = urlparse(url).path.replace(\"/\", \"_\")\n",
        "    if not filename or filename == \"_\":\n",
        "        filename = \"home\"\n",
        "    with open(os.path.join(folder, f\"{filename}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"URL: {url}\\n\\n{text}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# Crawl function with Requests\n",
        "# --------------------------------------\n",
        "def crawl_static(url, visited, depth=0, max_depth=2):\n",
        "    if url in visited or depth > max_depth:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    try:\n",
        "        print(f\"[Requests] Fetching: {url}\")\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch {url}: Status code {response.status_code}\")\n",
        "            return\n",
        "        text = clean_text(response.text)\n",
        "        save_text(url, text)\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {url} with error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Recursively crawl other internal links\n",
        "    base = \"{0.scheme}://{0.netloc}\".format(urlparse(url))\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        abs_url = urljoin(base, href)\n",
        "        if urlparse(abs_url).netloc == urlparse(url).netloc and abs_url.startswith(\"http\"):\n",
        "            crawl_static(abs_url, visited, depth + 1, max_depth)\n",
        "\n",
        "# --------------------------------------\n",
        "# MAIN\n",
        "# --------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start_url = \"https://www.mosdac.gov.in\"\n",
        "    visited = set()\n",
        "    crawl_static(start_url, visited, max_depth=2)  # You can increase depth for more pages\n",
        "    print(\"\\n✅ Extraction completed. All texts saved in 'extracted_texts/' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuyamZKrGAKt",
        "outputId": "4c4b06a0-d12e-4a38-b900-6c4b0f2caac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Requests] Fetching: https://www.mosdac.gov.in\n",
            "[Requests] Fetching: https://www.mosdac.gov.in#main-content\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/registration\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/uops\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/logout\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/insat-3dr\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/insat-3d\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/kalpana-1\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/insat-3a\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/megha-tropiques\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/saral-altika\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/oceansat-2\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/oceansat-3\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/insat-3ds\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/scatsat-1\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/catalog-satellite\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/catalog-insitu\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/catalog-radar\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/gallery\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/gallery/weather\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/gallery/ocean\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/gallery/dwr\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/gallery/current\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/bayesian-based-mt-saphir-rainfall\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/gps-derived-integrated-water-vapour\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/gsmap-isro-rain\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/meteosat8-cloud-properties\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/3d-volumetric-terls-dwrproduct\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/inland-water-height\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/river-discharge\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/soil-moisture-0\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/global-ocean-surface-current\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/high-resolution-sea-surface-salinity\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/indian-mainland-coastal-product\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/ocean-subsurface\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/oceanic-eddies-detection\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sea-ice-occurrence-probability\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/wave-based-renewable-energy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/calval-data\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/forecast-menu\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/rss-feed\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/insitu\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/calibration-reports\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/validation-reports\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/data-quality\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/weather-reports\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/atlases\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/tools\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sitemap\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/help\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-latest_products=0#qt-latest_products\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-latest_products=1#qt-latest_products\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-latest_products=2#qt-latest_products\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-latest_products=3#qt-latest_products\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-latest_products=4#qt-latest_products\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/imageshow\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/gallery/index.html%3F%26prod%3D3SIMG_%2A_L1B_STD_IR1_V%2A.jpg\n",
            "Failed to fetch https://www.mosdac.gov.in/gallery/index.html%3F%26prod%3D3SIMG_%2A_L1B_STD_IR1_V%2A.jpg: Status code 404\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/gallery/index.html?&prod=3SIMG_%2A_L1B_STD_IR1_V%2A.jpg\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=0#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=1#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=2#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=3#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=4#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/node?qt-services_quicktab=5#qt-services_quicktab\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/weather\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/coldwave\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/cyclone\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/heat-cold-wave\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/forecast-heavyrain\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/lightning\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/monsoon\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/sea-state-forecast\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/energy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/nowcast-cloudburst\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/nowcast-heavyrain\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/aws\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/event-heavyrain\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/live\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/state\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/afs-seac\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/urja\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/varsha\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/vayu\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/soil-wetness\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/oilspill\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/ocean\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/rip\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/ocean-eye\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/internal/eddy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sites/default/files/docs/Onset%20Prediction%202024.pdf\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sites/default/files/docs/Onset%20Prediction%202023.pdf\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sites/default/files/docs/INSAT_Product_Version_information_V01.pdf\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/sites/default/files/docs/sftp-mosdac_0.pdf\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/announcements\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/rss.xml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-1925126326.py:12: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_content, 'lxml')\n",
            "/tmp/ipython-input-2-1925126326.py:46: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(response.text, 'lxml')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Requests] Fetching: https://www.mosdac.gov.in/docs/STQC.pdf\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/mosdac-feedback\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/about-us\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/contact-us\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/copyright-policy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/data-access-policy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/hyperlink-policy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/privacy-policy\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/website-policies\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/terms-conditions\n",
            "[Requests] Fetching: https://www.mosdac.gov.in/faq-page\n",
            "\n",
            "✅ Extraction completed. All texts saved in 'extracted_texts/' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_all_texts(folder=\"extracted_texts\"):\n",
        "    all_docs = []\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                all_docs.append(f.read())\n",
        "    return all_docs\n",
        "\n",
        "documents = load_all_texts()"
      ],
      "metadata": {
        "id": "PostyVnwHCrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 1500000  # Max document length\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_triplets_from_chunk(chunk):\n",
        "    doc = nlp(chunk)\n",
        "    triplets = []\n",
        "    for sent in doc.sents:\n",
        "        subj = \"\"\n",
        "        obj = \"\"\n",
        "        verb = \"\"\n",
        "        for token in sent:\n",
        "            if \"subj\" in token.dep_:\n",
        "                subj = token.text\n",
        "            if \"obj\" in token.dep_:\n",
        "                obj = token.text\n",
        "            if token.pos_ == \"VERB\":\n",
        "                verb = token.lemma_\n",
        "        if subj and obj and verb:\n",
        "            triplets.append((subj, verb, obj))\n",
        "    return triplets\n",
        "\n",
        "# Final list of all triplets\n",
        "all_triplets = []\n",
        "\n",
        "# Process in chunks\n",
        "for doc in tqdm(documents):\n",
        "    # Split each document into smaller chunks (e.g., 1000 characters)\n",
        "    chunks = [doc[i:i+1000] for i in range(0, len(doc), 1000)]\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            all_triplets.extend(extract_triplets_from_chunk(chunk))\n",
        "        except Exception as e:\n",
        "            print(\"Skipping a chunk due to error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24bDY0xpHFRp",
        "outputId": "f0d3f80c-a4ea-4c3c-903d-e13f10ecc72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [05:02<00:00,  3.22s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total triplets extracted: {len(all_triplets)}\")\n",
        "print(\"Sample triplets:\", all_triplets[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsoo6VIZNvEn",
        "outputId": "4c1f55e9-d10b-4917-c43b-34d0b1e97b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total triplets extracted: 1001\n",
            "Sample triplets: [('Alerts', 'IST', 'NOWCAST'), ('Legends', 'use', 'satellites'), ('alerts', 'use', 'events'), ('Heavyrain', 'attach', 'document'), ('information', 'use', 'interpretation'), ('MOSDAC', 'incur', 'loss'), ('representations', 'mention', 'purpose'), ('we', 'include', 'limitation'), ('effort', 'run', 'website'), ('website', 'take', 'control')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_kg(triplets):\n",
        "    G = nx.DiGraph()\n",
        "    for subj, rel, obj in triplets:\n",
        "        G.add_edge(subj, obj, label=rel)\n",
        "    return G\n",
        "\n",
        "G = build_kg(all_triplets)"
      ],
      "metadata": {
        "id": "ffz-GNxCHHoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nodes in KG:\", G.number_of_nodes())\n",
        "print(\"Edges in KG:\", G.number_of_edges())\n",
        "\n",
        "# Optional: list a few\n",
        "print(\"Sample edges:\", list(G.edges(data=True))[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSrZtJilN2n_",
        "outputId": "8a6bef5a-176a-4bae-9c1e-399f3b0090f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes in KG: 693\n",
            "Edges in KG: 660\n",
            "Sample edges: [('Alerts', 'NOWCAST', {'label': 'IST'}), ('Alerts', 'grid', {'label': 'update'}), ('Legends', 'satellites', {'label': 'use'}), ('alerts', 'events', {'label': 'use'}), ('Heavyrain', 'document', {'label': 'attach'}), ('information', 'interpretation', {'label': 'use'}), ('information', 'development', {'label': 'contain'}), ('information', 'weather', {'label': 'use'}), ('information', 'Chanderet', {'label': 'find'}), ('information', 'pulse', {'label': 'base'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    doc = nlp(question)\n",
        "    subj = None\n",
        "    verb = None\n",
        "\n",
        "    for token in doc:\n",
        "        if \"subj\" in token.dep_ or token.dep_ == \"nsubj\":\n",
        "            subj = token.lemma_.lower()\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            verb = token.lemma_.lower()\n",
        "\n",
        "    answers = []\n",
        "\n",
        "    for u, v, d in G.edges(data=True):\n",
        "        if u.lower() == subj and d['label'].lower() == verb:\n",
        "            answers.append(v)\n",
        "\n",
        "    return answers if answers else [\"Sorry, I couldn't find the answer.\"]"
      ],
      "metadata": {
        "id": "oDOV1jq1J7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_qa(test_questions):\n",
        "    correct = 0\n",
        "    for q, expected_answers in test_questions:\n",
        "        pred = answer_question(q)\n",
        "        match = any(ans.lower() in [p.lower() for p in pred] for ans in expected_answers)\n",
        "\n",
        "        print(f\"\\n Question: {q}\")\n",
        "        print(f\"   ➤ Expected: {expected_answers}\")\n",
        "        print(f\"   ➤ Got: {pred}\")\n",
        "        print(f\"   ✅ {'Correct' if match else 'Incorrect'}\")\n",
        "\n",
        "        if match:\n",
        "            correct += 1\n",
        "\n",
        "    total = len(test_questions)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\n📊 QA Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "Y9gSeR1-J-ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    (\"What does information contain?\", [\"development\"]),\n",
        "    (\"What does information use?\", [\"weather\", \"interpretation\"]),\n",
        "    (\"What does alerts update?\", [\"grid\"]),\n",
        "    (\"What does alerts use?\", [\"events\"]),\n",
        "    (\"What does legends use?\", [\"satellites\"]),\n",
        "    (\"What does heavyrain attach?\", [\"document\"]),\n",
        "]"
      ],
      "metadata": {
        "id": "Q9HtyVMEPS9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_qa(test_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQynnQmvPV0x",
        "outputId": "5afe1227-dca9-42f1-9d1a-1bf387c16380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❓ Question: What does information contain?\n",
            "   ➤ Expected: ['development']\n",
            "   ➤ Got: ['development']\n",
            "   ✅ Correct\n",
            "\n",
            "❓ Question: What does information use?\n",
            "   ➤ Expected: ['weather', 'interpretation']\n",
            "   ➤ Got: [\"Sorry, I couldn't find the answer.\"]\n",
            "   ✅ Incorrect\n",
            "\n",
            "❓ Question: What does alerts update?\n",
            "   ➤ Expected: ['grid']\n",
            "   ➤ Got: [\"Sorry, I couldn't find the answer.\"]\n",
            "   ✅ Incorrect\n",
            "\n",
            "❓ Question: What does alerts use?\n",
            "   ➤ Expected: ['events']\n",
            "   ➤ Got: [\"Sorry, I couldn't find the answer.\"]\n",
            "   ✅ Incorrect\n",
            "\n",
            "❓ Question: What does legends use?\n",
            "   ➤ Expected: ['satellites']\n",
            "   ➤ Got: [\"Sorry, I couldn't find the answer.\"]\n",
            "   ✅ Incorrect\n",
            "\n",
            "❓ Question: What does heavyrain attach?\n",
            "   ➤ Expected: ['document']\n",
            "   ➤ Got: ['document']\n",
            "   ✅ Correct\n",
            "\n",
            "📊 QA Accuracy: 33.33%\n"
          ]
        }
      ]
    }
  ]
}