{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff2632a2",
        "outputId": "eb8c71f6-13f9-442d-b5f7-5cce565e4ab8"
      },
      "source": [
        "!pip install requests beautifulsoup4 reportlab pillow pytesseract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from reportlab.lib.pagesizes import letter, A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\n",
        "from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_JUSTIFY\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Optional imports for advanced text extraction\n",
        "try:\n",
        "    import pytesseract\n",
        "    from PIL import Image, ImageEnhance, ImageFilter\n",
        "    OCR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OCR_AVAILABLE = False\n",
        "    print(\"⚠️ OCR not available. Install pytesseract and Pillow for image text extraction\")\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    CV2_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CV2_AVAILABLE = False\n",
        "    print(\"⚠️ OpenCV not available. Install opencv-python for advanced image processing\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MOSDACTextExtractor:\n",
        "    def __init__(self, base_url, output_folder=\"mosdac_text_data\", max_depth=2):\n",
        "        self.base_url = base_url\n",
        "        self.output_folder = output_folder\n",
        "        self.max_depth = max_depth\n",
        "        self.visited_urls = set()\n",
        "        self.all_text_content = []\n",
        "        self.temp_folder = os.path.join(output_folder, \"temp\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        os.makedirs(self.temp_folder, exist_ok=True)\n",
        "\n",
        "    def clean_text(self, html_content):\n",
        "        \"\"\"Clean and extract meaningful text from HTML\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        # Remove unwanted elements\n",
        "        for element in soup([\"script\", \"style\", \"noscript\", \"nav\", \"footer\", \"header\"]):\n",
        "            element.extract()\n",
        "\n",
        "        # Get text with better formatting\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_text_from_image(self, img_url, img_element):\n",
        "        \"\"\"Extract text from images using OCR and context analysis\"\"\"\n",
        "        extracted_text = \"\"\n",
        "\n",
        "        try:\n",
        "            # Download image\n",
        "            response = requests.get(img_url, timeout=10, stream=True)\n",
        "            if response.status_code != 200:\n",
        "                return f\"[Image: {img_url} - Could not download]\"\n",
        "\n",
        "            # Open image\n",
        "            image = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Get image context from HTML\n",
        "            alt_text = img_element.get('alt', '')\n",
        "            title_text = img_element.get('title', '')\n",
        "\n",
        "            # Check if image might contain text/data\n",
        "            width, height = image.size\n",
        "\n",
        "            # Analyze image type based on context and attributes\n",
        "            img_description = self.analyze_image_content(image, img_url, alt_text, title_text)\n",
        "\n",
        "            # Try OCR if available and image seems to contain text\n",
        "            if OCR_AVAILABLE and self.might_contain_text(img_url, alt_text, title_text):\n",
        "                try:\n",
        "                    # Enhance image for better OCR\n",
        "                    enhanced_image = self.enhance_image_for_ocr(image)\n",
        "                    ocr_text = pytesseract.image_to_string(enhanced_image, lang='eng')\n",
        "\n",
        "                    if ocr_text.strip() and len(ocr_text.strip()) > 5:\n",
        "                        img_description += f\"\\n\\nText extracted from image: {ocr_text.strip()}\"\n",
        "\n",
        "                except Exception as ocr_error:\n",
        "                    logger.debug(f\"OCR failed for {img_url}: {ocr_error}\")\n",
        "\n",
        "            extracted_text = img_description\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing image {img_url}: {e}\")\n",
        "            extracted_text = f\"[Image: {img_url} - Processing failed]\"\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def might_contain_text(self, img_url, alt_text, title_text):\n",
        "        \"\"\"Determine if image might contain text worth extracting\"\"\"\n",
        "        text_indicators = [\n",
        "            'chart', 'graph', 'diagram', 'table', 'data', 'statistics',\n",
        "            'figure', 'plot', 'map', 'infographic', 'screenshot',\n",
        "            'document', 'text', 'caption', 'label', 'title'\n",
        "        ]\n",
        "\n",
        "        combined_text = f\"{img_url} {alt_text} {title_text}\".lower()\n",
        "\n",
        "        return any(indicator in combined_text for indicator in text_indicators)\n",
        "\n",
        "    def enhance_image_for_ocr(self, image):\n",
        "        \"\"\"Enhance image quality for better OCR results\"\"\"\n",
        "        try:\n",
        "            # Convert to grayscale\n",
        "            if image.mode != 'L':\n",
        "                image = image.convert('L')\n",
        "\n",
        "            # Enhance contrast\n",
        "            enhancer = ImageEnhance.Contrast(image)\n",
        "            image = enhancer.enhance(2.0)\n",
        "\n",
        "            # Enhance sharpness\n",
        "            enhancer = ImageEnhance.Sharpness(image)\n",
        "            image = enhancer.enhance(2.0)\n",
        "\n",
        "            # Resize if too small\n",
        "            width, height = image.size\n",
        "            if width < 300 or height < 300:\n",
        "                scale = max(300/width, 300/height)\n",
        "                new_size = (int(width * scale), int(height * scale))\n",
        "                image = image.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "            return image\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Image enhancement failed: {e}\")\n",
        "            return image\n",
        "\n",
        "    def analyze_image_content(self, image, img_url, alt_text, title_text):\n",
        "        \"\"\"Analyze image content and generate descriptive text\"\"\"\n",
        "        description_parts = []\n",
        "\n",
        "        # Basic image info\n",
        "        width, height = image.size\n",
        "        mode = image.mode\n",
        "\n",
        "        description_parts.append(f\"[IMAGE ANALYSIS]\")\n",
        "        description_parts.append(f\"Source: {img_url}\")\n",
        "        description_parts.append(f\"Dimensions: {width}x{height} pixels\")\n",
        "\n",
        "        # Add alt text and title if available\n",
        "        if alt_text:\n",
        "            description_parts.append(f\"Alt text: {alt_text}\")\n",
        "        if title_text:\n",
        "            description_parts.append(f\"Title: {title_text}\")\n",
        "\n",
        "        # Analyze image type based on URL and context\n",
        "        img_type = self.classify_image_type(img_url, alt_text, title_text)\n",
        "        description_parts.append(f\"Likely content type: {img_type}\")\n",
        "\n",
        "        # Analyze colors and composition\n",
        "        try:\n",
        "            color_analysis = self.analyze_image_colors(image)\n",
        "            description_parts.append(f\"Color analysis: {color_analysis}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Check for common patterns\n",
        "        pattern_analysis = self.detect_image_patterns(img_url, alt_text, title_text, width, height)\n",
        "        if pattern_analysis:\n",
        "            description_parts.append(f\"Pattern analysis: {pattern_analysis}\")\n",
        "\n",
        "        return \"\\n\".join(description_parts)\n",
        "\n",
        "    def classify_image_type(self, img_url, alt_text, title_text):\n",
        "        \"\"\"Classify image type based on available information\"\"\"\n",
        "        combined_text = f\"{img_url} {alt_text} {title_text}\".lower()\n",
        "\n",
        "        if any(word in combined_text for word in ['logo', 'brand', 'header']):\n",
        "            return \"Logo/Branding element\"\n",
        "        elif any(word in combined_text for word in ['chart', 'graph', 'plot']):\n",
        "            return \"Data visualization (chart/graph)\"\n",
        "        elif any(word in combined_text for word in ['map', 'satellite', 'geographic']):\n",
        "            return \"Geographic/Satellite imagery\"\n",
        "        elif any(word in combined_text for word in ['diagram', 'flowchart', 'schema']):\n",
        "            return \"Technical diagram/flowchart\"\n",
        "        elif any(word in combined_text for word in ['screenshot', 'interface', 'ui']):\n",
        "            return \"User interface screenshot\"\n",
        "        elif any(word in combined_text for word in ['photo', 'picture', 'image']):\n",
        "            return \"Photograph/Image\"\n",
        "        elif any(word in combined_text for word in ['icon', 'button', 'symbol']):\n",
        "            return \"Icon/Symbol\"\n",
        "        else:\n",
        "            return \"General image content\"\n",
        "\n",
        "    def analyze_image_colors(self, image):\n",
        "        \"\"\"Analyze dominant colors in the image\"\"\"\n",
        "        try:\n",
        "            # Convert to RGB if needed\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "            # Get image data\n",
        "            image_data = list(image.getdata())\n",
        "\n",
        "            # Sample colors (take every 100th pixel to avoid performance issues)\n",
        "            sampled_colors = image_data[::100]\n",
        "\n",
        "            # Analyze brightness\n",
        "            brightness_values = [sum(pixel)/3 for pixel in sampled_colors if len(pixel) >= 3]\n",
        "            avg_brightness = sum(brightness_values) / len(brightness_values) if brightness_values else 128\n",
        "\n",
        "            if avg_brightness > 200:\n",
        "                return \"Predominantly light/bright image\"\n",
        "            elif avg_brightness < 80:\n",
        "                return \"Predominantly dark image\"\n",
        "            else:\n",
        "                return \"Mixed brightness levels\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return \"Color analysis unavailable\"\n",
        "\n",
        "    def detect_image_patterns(self, img_url, alt_text, title_text, width, height):\n",
        "        \"\"\"Detect common patterns in images\"\"\"\n",
        "        patterns = []\n",
        "\n",
        "        # Aspect ratio analysis\n",
        "        aspect_ratio = width / height if height > 0 else 1\n",
        "\n",
        "        if aspect_ratio > 2:\n",
        "            patterns.append(\"Wide format (possibly banner or header)\")\n",
        "        elif aspect_ratio < 0.5:\n",
        "            patterns.append(\"Tall format (possibly sidebar or vertical chart)\")\n",
        "        elif 0.9 <= aspect_ratio <= 1.1:\n",
        "            patterns.append(\"Square format\")\n",
        "\n",
        "        # Size analysis\n",
        "        if width * height > 1000000:  # > 1MP\n",
        "            patterns.append(\"High resolution image\")\n",
        "        elif width * height < 10000:  # < 10K pixels\n",
        "            patterns.append(\"Small icon or thumbnail\")\n",
        "\n",
        "        return \"; \".join(patterns) if patterns else None\n",
        "\n",
        "    def extract_table_text(self, soup, page_url):\n",
        "        \"\"\"Extract and format table content as readable text\"\"\"\n",
        "        tables_text = []\n",
        "\n",
        "        for i, table in enumerate(soup.find_all('table')):\n",
        "            try:\n",
        "                table_text = [f\"\\n[TABLE {i+1} FROM {page_url}]\"]\n",
        "\n",
        "                # Extract caption if available\n",
        "                caption = table.find('caption')\n",
        "                if caption:\n",
        "                    table_text.append(f\"Table Caption: {caption.get_text(strip=True)}\")\n",
        "\n",
        "                # Extract table rows\n",
        "                rows = table.find_all('tr')\n",
        "                for row_idx, row in enumerate(rows):\n",
        "                    cells = row.find_all(['td', 'th'])\n",
        "                    if cells:\n",
        "                        row_data = []\n",
        "                        for cell in cells:\n",
        "                            cell_text = cell.get_text(strip=True)\n",
        "                            if cell_text:\n",
        "                                row_data.append(cell_text)\n",
        "\n",
        "                        if row_data:\n",
        "                            if row_idx == 0:  # Header row\n",
        "                                table_text.append(f\"Headers: {' | '.join(row_data)}\")\n",
        "                            else:\n",
        "                                table_text.append(f\"Row {row_idx}: {' | '.join(row_data)}\")\n",
        "\n",
        "                if len(table_text) > 1:  # Only add if we found actual content\n",
        "                    tables_text.append(\"\\n\".join(table_text))\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error extracting table: {e}\")\n",
        "                continue\n",
        "\n",
        "        return \"\\n\\n\".join(tables_text) if tables_text else \"\"\n",
        "\n",
        "    def extract_list_text(self, soup):\n",
        "        \"\"\"Extract and format list content\"\"\"\n",
        "        lists_text = []\n",
        "\n",
        "        # Extract ordered and unordered lists\n",
        "        for list_type in ['ul', 'ol']:\n",
        "            for i, list_elem in enumerate(soup.find_all(list_type)):\n",
        "                list_items = list_elem.find_all('li')\n",
        "                if list_items:\n",
        "                    list_text = [f\"\\n[{list_type.upper()} LIST {i+1}]\"]\n",
        "                    for idx, item in enumerate(list_items, 1):\n",
        "                        item_text = item.get_text(strip=True)\n",
        "                        if item_text:\n",
        "                            prefix = f\"{idx}.\" if list_type == 'ol' else \"•\"\n",
        "                            list_text.append(f\"{prefix} {item_text}\")\n",
        "\n",
        "                    if len(list_text) > 1:\n",
        "                        lists_text.append(\"\\n\".join(list_text))\n",
        "\n",
        "        return \"\\n\\n\".join(lists_text) if lists_text else \"\"\n",
        "\n",
        "    def extract_metadata_text(self, soup, page_url):\n",
        "        \"\"\"Extract metadata as readable text\"\"\"\n",
        "        metadata_text = [f\"\\n[PAGE METADATA FOR {page_url}]\"]\n",
        "\n",
        "        # Title\n",
        "        title = soup.find('title')\n",
        "        if title:\n",
        "            metadata_text.append(f\"Page Title: {title.get_text(strip=True)}\")\n",
        "\n",
        "        # Meta description\n",
        "        description_meta = soup.find('meta', attrs={'name': 'description'})\n",
        "        if description_meta:\n",
        "            metadata_text.append(f\"Description: {description_meta.get('content', '')}\")\n",
        "\n",
        "        # Meta keywords\n",
        "        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})\n",
        "        if keywords_meta:\n",
        "            metadata_text.append(f\"Keywords: {keywords_meta.get('content', '')}\")\n",
        "\n",
        "        # Headings structure\n",
        "        headings = []\n",
        "        for h_level in range(1, 7):\n",
        "            h_tags = soup.find_all(f'h{h_level}')\n",
        "            for h_tag in h_tags:\n",
        "                heading_text = h_tag.get_text(strip=True)\n",
        "                if heading_text:\n",
        "                    headings.append(f\"H{h_level}: {heading_text}\")\n",
        "\n",
        "        if headings:\n",
        "            metadata_text.append(\"Page Structure:\")\n",
        "            metadata_text.extend(headings)\n",
        "\n",
        "        return \"\\n\".join(metadata_text) if len(metadata_text) > 1 else \"\"\n",
        "\n",
        "    def crawl_static(self, url, visited, depth=0, max_depth=2):\n",
        "        \"\"\"Enhanced crawl function that extracts all content as text\"\"\"\n",
        "        if url in visited or depth > max_depth:\n",
        "            return\n",
        "\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Crawling: {url} (depth: {depth})\")\n",
        "            response = requests.get(url, timeout=15, headers={\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            })\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                logger.warning(f\"Failed to fetch {url}: Status {response.status_code}\")\n",
        "                return\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "            # Collect all text content\n",
        "            page_content = []\n",
        "\n",
        "            # Add page header\n",
        "            page_content.append(f\"\\n{'='*80}\")\n",
        "            page_content.append(f\"PAGE: {url}\")\n",
        "            page_content.append(f\"CRAWLED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            page_content.append(f\"DEPTH: {depth}\")\n",
        "            page_content.append(f\"{'='*80}\\n\")\n",
        "\n",
        "            # Extract main text content\n",
        "            main_text = self.clean_text(response.text)\n",
        "            if main_text.strip():\n",
        "                page_content.append(\"[MAIN CONTENT]\")\n",
        "                page_content.append(main_text)\n",
        "                page_content.append(\"\")\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata_text = self.extract_metadata_text(soup, url)\n",
        "            if metadata_text:\n",
        "                page_content.append(metadata_text)\n",
        "                page_content.append(\"\")\n",
        "\n",
        "            # Extract table content\n",
        "            tables_text = self.extract_table_text(soup, url)\n",
        "            if tables_text:\n",
        "                page_content.append(tables_text)\n",
        "                page_content.append(\"\")\n",
        "\n",
        "            # Extract list content\n",
        "            lists_text = self.extract_list_text(soup)\n",
        "            if lists_text:\n",
        "                page_content.append(lists_text)\n",
        "                page_content.append(\"\")\n",
        "\n",
        "            # Extract text from images\n",
        "            base_url = \"{0.scheme}://{0.netloc}\".format(urlparse(url))\n",
        "            images_text = []\n",
        "\n",
        "            for img in soup.find_all('img'):\n",
        "                img_src = img.get('src')\n",
        "                if img_src:\n",
        "                    img_url = urljoin(base_url, img_src)\n",
        "                    if not img_url.startswith('data:'):  # Skip data URLs\n",
        "                        img_text = self.extract_text_from_image(img_url, img)\n",
        "                        if img_text:\n",
        "                            images_text.append(img_text)\n",
        "                            # Add delay to be respectful\n",
        "                            time.sleep(0.5)\n",
        "\n",
        "            if images_text:\n",
        "                page_content.append(\"\\n[IMAGES AND VISUAL CONTENT]\")\n",
        "                page_content.extend(images_text)\n",
        "                page_content.append(\"\")\n",
        "\n",
        "            # Combine all content\n",
        "            full_page_content = \"\\n\".join(page_content)\n",
        "\n",
        "            # Store the content\n",
        "            self.all_text_content.append({\n",
        "                'url': url,\n",
        "                'content': full_page_content,\n",
        "                'extracted_at': datetime.now().isoformat(),\n",
        "                'depth': depth\n",
        "            })\n",
        "\n",
        "            # Save individual text file\n",
        "            filename = urlparse(url).path.replace(\"/\", \"_\") or \"home\"\n",
        "            text_path = os.path.join(self.output_folder, f\"{filename}_depth{depth}.txt\")\n",
        "            with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(full_page_content)\n",
        "\n",
        "            logger.info(f\"Extracted {len(full_page_content)} characters from {url}\")\n",
        "\n",
        "            # Recursively crawl other internal links\n",
        "            if depth < max_depth:\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link['href']\n",
        "                    abs_url = urljoin(base_url, href)\n",
        "\n",
        "                    # Only crawl internal links\n",
        "                    if (urlparse(abs_url).netloc == urlparse(url).netloc and\n",
        "                        abs_url.startswith(\"http\") and abs_url not in visited):\n",
        "\n",
        "                        # Add delay between requests\n",
        "                        time.sleep(2)\n",
        "                        self.crawl_static(abs_url, visited, depth + 1, max_depth)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "    def generate_pdf_report(self, output_filename=\"MOSDAC.pdf\"):\n",
        "        \"\"\"Generate text-only PDF report\"\"\"\n",
        "        pdf_path = os.path.join(self.output_folder, output_filename)\n",
        "        doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "        styles = getSampleStyleSheet()\n",
        "        story = []\n",
        "\n",
        "        # Custom styles\n",
        "        title_style = ParagraphStyle(\n",
        "            'CustomTitle',\n",
        "            parent=styles['Title'],\n",
        "            fontSize=20,\n",
        "            spaceAfter=30,\n",
        "            alignment=TA_CENTER\n",
        "        )\n",
        "\n",
        "        # Title page\n",
        "        story.append(Paragraph(\"MOSDAC Complete Text Data Extraction\", title_style))\n",
        "        story.append(Spacer(1, 20))\n",
        "        story.append(Paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", styles['Normal']))\n",
        "        story.append(Paragraph(f\"Base URL: {self.base_url}\", styles['Normal']))\n",
        "        story.append(Paragraph(f\"Total Pages: {len(self.all_text_content)}\", styles['Normal']))\n",
        "        story.append(Paragraph(f\"Max Depth: {self.max_depth}\", styles['Normal']))\n",
        "        story.append(Spacer(1, 30))\n",
        "\n",
        "        # Summary\n",
        "        total_words = sum(len(page['content'].split()) for page in self.all_text_content)\n",
        "        story.append(Paragraph(f\"Total Words Extracted: {total_words:,}\", styles['Heading2']))\n",
        "        story.append(Paragraph(\"This document contains all textual content extracted from the MOSDAC website, including text from images, tables, metadata, and visual content descriptions.\", styles['Normal']))\n",
        "\n",
        "        story.append(PageBreak())\n",
        "\n",
        "        # Add all content\n",
        "        for i, page_data in enumerate(self.all_text_content):\n",
        "            try:\n",
        "                # Convert content to paragraphs for PDF\n",
        "                content_lines = page_data['content'].split('\\n')\n",
        "\n",
        "                for line in content_lines:\n",
        "                    if line.strip():\n",
        "                        # Handle different formatting\n",
        "                        if line.startswith('='):\n",
        "                            continue  # Skip separator lines\n",
        "                        elif line.startswith('[') and line.endswith(']'):\n",
        "                            # Section headers\n",
        "                            story.append(Paragraph(line, styles['Heading3']))\n",
        "                        elif line.startswith('PAGE:'):\n",
        "                            story.append(Paragraph(line, styles['Heading2']))\n",
        "                        else:\n",
        "                            # Regular content\n",
        "                            # Escape HTML characters and handle long lines\n",
        "                            escaped_line = line.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
        "                            if len(escaped_line) > 500:\n",
        "                                # Break very long lines\n",
        "                                words = escaped_line.split()\n",
        "                                current_line = []\n",
        "                                for word in words:\n",
        "                                    current_line.append(word)\n",
        "                                    if len(' '.join(current_line)) > 400:\n",
        "                                        story.append(Paragraph(' '.join(current_line), styles['Normal']))\n",
        "                                        current_line = []\n",
        "                                if current_line:\n",
        "                                    story.append(Paragraph(' '.join(current_line), styles['Normal']))\n",
        "                            else:\n",
        "                                story.append(Paragraph(escaped_line, styles['Normal']))\n",
        "                    else:\n",
        "                        story.append(Spacer(1, 6))\n",
        "\n",
        "                # Add page break after every few pages to manage PDF size\n",
        "                if (i + 1) % 5 == 0:\n",
        "                    story.append(PageBreak())\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error adding content to PDF: {e}\")\n",
        "                story.append(Paragraph(f\"[Error processing content from {page_data['url']}]\", styles['Normal']))\n",
        "\n",
        "        # Build PDF\n",
        "        try:\n",
        "            doc.build(story)\n",
        "            logger.info(f\"PDF report generated successfully: {pdf_path}\")\n",
        "            return pdf_path\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating PDF: {e}\")\n",
        "            return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    start_url = \"https://www.mosdac.gov.in\"\n",
        "\n",
        "    print(\"Starting MOSDAC comprehensive text extraction...\")\n",
        "    print(f\"Target URL: {start_url}\")\n",
        "    print(f\"OCR Available: {OCR_AVAILABLE}\")\n",
        "    print(f\"OpenCV Available: {CV2_AVAILABLE}\")\n",
        "    print(f\"Timestamp: {datetime.now()}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Initialize extractor\n",
        "    extractor = MOSDACTextExtractor(start_url, max_depth=2)\n",
        "\n",
        "    # Start extraction\n",
        "    start_time = time.time()\n",
        "    visited = set()\n",
        "    extractor.crawl_static(start_url, visited, max_depth=2)\n",
        "\n",
        "    # Generate PDF report\n",
        "    print(\"\\n Generating comprehensive PDF report...\")\n",
        "    pdf_path = extractor.generate_pdf_report(\"MOSDAC.pdf\")\n",
        "\n",
        "    # Save combined text file as backup\n",
        "    all_text = \"\\n\\n\".join(page['content'] for page in extractor.all_text_content)\n",
        "    backup_path = os.path.join(extractor.output_folder, \"MOSDAC_complete_text.txt\")\n",
        "    with open(backup_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(all_text)\n",
        "\n",
        "    # Print summary\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    total_words = sum(len(page['content'].split()) for page in extractor.all_text_content)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TEXT EXTRACTION COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\" Pages processed: {len(extractor.all_text_content)}\")\n",
        "    print(f\" Total words extracted: {total_words:,}\")\n",
        "    print(f\" URLs visited: {len(visited)}\")\n",
        "    print(f\"  Time taken: {duration:.2f} seconds\")\n",
        "    print(f\" Output folder: {extractor.output_folder}\")\n",
        "\n",
        "    if pdf_path:\n",
        "        print(f\" Main PDF: {pdf_path}\")\n",
        "\n",
        "    print(\"\\n Comprehensive text data ready for bot training!\")\n",
        "    print(\"   - All visual content analyzed and converted to text\")\n",
        "    print(\"   - Tables formatted as readable text\")\n",
        "    print(\"   - Images analyzed with OCR where applicable\")\n",
        "    print(\"   - Complete metadata extraction\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0OTZMNwcIWL",
        "outputId": "876dd747-bb06-460e-f1f1-ccb3e0204223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MOSDAC comprehensive text extraction...\n",
            "Target URL: https://www.mosdac.gov.in\n",
            "OCR Available: True\n",
            "OpenCV Available: True\n",
            "Timestamp: 2025-08-22 15:44:49.701796\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Failed to fetch https://www.mosdac.gov.in/gallery/index.html%3F%26prod%3D3SIMG_%2A_L1B_STD_IR1_V%2A.jpg: Status 404\n",
            "/tmp/ipython-input-3919527048.py:369: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(response.text, 'lxml')\n",
            "/tmp/ipython-input-3919527048.py:55: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_content, 'lxml')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generating comprehensive PDF report...\n",
            "\n",
            "======================================================================\n",
            " TEXT EXTRACTION COMPLETED!\n",
            "======================================================================\n",
            " Pages processed: 107\n",
            " Total words extracted: 429,954\n",
            " URLs visited: 108\n",
            "  Time taken: 2367.43 seconds\n",
            " Output folder: mosdac_text_data\n",
            " Main PDF: mosdac_text_data/MOSDAC.pdf\n",
            "\n",
            " Comprehensive text data ready for bot training!\n",
            "   - All visual content analyzed and converted to text\n",
            "   - Tables formatted as readable text\n",
            "   - Images analyzed with OCR where applicable\n",
            "   - Complete metadata extraction\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}